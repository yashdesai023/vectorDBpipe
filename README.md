<div align="center">

<img src="https://readme-typing-svg.demolab.com?font=Fira+Code&size=30&pause=1000&color=6C63FF&center=true&vCenter=true&width=600&lines=vectorDBpipe+;All-in-One+Enterprise+RAG+Engine;Tri-Processing+%7C+4+AI+Engines+%7C+15%2B+Sources" alt="Typing SVG"/>

<h1> vectorDBpipe</h1>

<p><strong>The All-in-One Enterprise RAG Engine with Omni-RAG Architecture</strong></p>

<p>
  <a href="https://badge.fury.io/py/vectordbpipe"><img src="https://badge.fury.io/py/vectordbpipe.svg" alt="PyPI version"/></a>
  <a href="https://www.python.org/downloads/"><img src="https://img.shields.io/badge/python-3.8%2B-blue.svg" alt="Python 3.8+"/></a>
  <a href="https://opensource.org/licenses/MIT"><img src="https://img.shields.io/badge/License-MIT-yellow.svg" alt="License: MIT"/></a>
  <a href="https://github.com/yashdesai023/vectorDBpipe/actions"><img src="https://github.com/yashdesai023/vectorDBpipe/actions/workflows/ci.yml/badge.svg" alt="CI"/></a>
  <img src="https://img.shields.io/badge/version-0.2.0-brightgreen.svg" alt="Version 0.2.0"/>
  <img src="https://img.shields.io/badge/tests-4%20passed-success.svg" alt="Tests 4 passed"/>
  <img src="https://img.shields.io/badge/PyPI-vectordbpipe-blueviolet.svg" alt="PyPI"/>
</p>

<p>
  <b>The only Python SDK you need for RAG ‚Äî Ingest once, query with 4 intelligent engines.</b>
</p>

</div>

---

## üìã Table of Contents

- [What is vectorDBpipe?](#-what-is-vectordbpipe)
- [Why vectorDBpipe?](#-why-vectordbpipe)
- [What's New in v0.2.0 ‚Äî Omni-RAG Architecture](#-whats-new-in-v020--omni-rag-architecture)
- [The 4 Omni-RAG Engines](#-the-4-omni-rag-engines)
- [Tri-Processing Ingestion Pipeline](#-tri-processing-ingestion-pipeline)
- [15+ Native Data Integrations](#-15-native-data-integrations)
- [Architecture Overview](#-architecture-overview)
- [Installation](#-installation)
- [Configuration](#-configuration)
- [Quickstart Guide](#-quickstart-guide)
- [Advanced Usage](#-advanced-usage)
- [API Reference](#-api-reference)
- [Performance Benchmarks](#-performance-benchmarks)
- [Repository Structure](#-repository-structure)
- [Running Tests](#-running-tests)
- [Contributing](#-contributing)
- [Changelog](#-changelog)
- [License](#-license)

---

## üß† What is vectorDBpipe?

**vectorDBpipe** is a production-ready, open-source Python SDK that unifies the entire modern RAG (Retrieval-Augmented Generation) stack into a single, intelligent pipeline.

Instead of gluing together multiple libraries ‚Äî `LlamaIndex` for standard RAG, Microsoft's `GraphRAG` for knowledge graph traversal, and custom `LangChain` chains for structured JSON extraction ‚Äî **vectorDBpipe** brings all four paradigms into one cohesive Python class: `VDBpipe`.

With a single `pip install`, you get:

- ‚úÖ **4 AI Engines** that automatically route based on query type
- ‚úÖ **Tri-Processing Ingestion** that builds vectors, page indexes, and knowledge graphs simultaneously
- ‚úÖ **15+ Native Data Loaders** for PDFs, S3, Notion, Slack, GitHub, and more
- ‚úÖ **Zero configuration required** ‚Äî works out of the box with sensible defaults

```python
from vectorDBpipe import VDBpipe

pipeline = VDBpipe()
pipeline.ingest("data/contracts/")
answer = pipeline.query("What is the penalty for late payment?")
```

That's genuinely all it takes.

---

## üåü Why vectorDBpipe?

### The Real Problem

The modern RAG landscape is deeply fragmented. A production AI application needs:

- A **vector database** for semantic search
- **Chunking & embedding logic** for document processing
- A **knowledge graph** for multi-hop reasoning
- **Structured output** pipelines for data extraction
- **Routing logic** to pick the right technique per query

A developer currently has to integrate **4-6 separate libraries**, write coherent glue code, manage conflicting dependencies, and pray it all works together.

### Our Solution: One SDK, Four Engines, Zero Fragmentation

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                          VDBpipe SDK                            ‚îÇ
‚îÇ  ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó  ‚îÇ
‚îÇ  ‚ïë               OmniRouter (Intelligent Dispatch)           ‚ïë  ‚îÇ
‚îÇ  ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï¶‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï¶‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï¶‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù  ‚îÇ
‚îÇ                ‚ñº              ‚ñº           ‚ñº           ‚ñº         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ
‚îÇ  ‚îÇ  Engine 1   ‚îÇ  ‚îÇ Engine 2 ‚îÇ  ‚îÇ Engine 3  ‚îÇ  ‚îÇ   Engine 4   ‚îÇ‚îÇ
‚îÇ  ‚îÇ Vector RAG  ‚îÇ  ‚îÇVectorless‚îÇ  ‚îÇ GraphRAG  ‚îÇ  ‚îÇLangChain     ‚îÇ‚îÇ
‚îÇ  ‚îÇ  (Fast)     ‚îÇ  ‚îÇ  (Deep)  ‚îÇ  ‚îÇ(Detective)‚îÇ  ‚îÇExtract (JSON)‚îÇ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üÜï What's New in v0.2.0 ‚Äî Omni-RAG Architecture

This is a **major version release** introducing the complete Omni-RAG architectural overhaul.

### ‚ú® New Features

| Feature | Description |
|---|---|
| üß† **OmniRouter** | Automatically classifies every query by intent and dispatches to the correct engine. No manual routing code needed. |
| ‚ö° **Tri-Processing Ingestion** | `ThreadPoolExecutor`-backed parallel pipeline. Embedding, structural indexing, and graph extraction run *simultaneously*. |
| üï∏Ô∏è **GraphRAG Engine (Engine 3)** | Integrated `NetworkX` for local knowledge graph storage. Entity-relationship pairs extracted and stored during ingestion. |
| üóÇÔ∏è **Vectorless Engine (Engine 2)** | A hierarchical `PageIndex` JSON structure allows holistic, chapter-level document comprehension without vector search. |
| üß© **LangChain JSON Extraction (Engine 4)** | Pydantic-schema-driven structured output. Pass a Python dictionary, get back type-safe JSON. |
| üîå **15+ Data Integrations** | Full loaders for PDF, DOCX, CSV, JSON, HTML, Markdown, XML, S3, Web URLs, and 6 SaaS connectors. |
| üß™ **PyTest Suite** | Comprehensive unit test coverage (`tests/test_vdbpipe.py`) with 4 core test cases using mocks. |
| üì¶ **Improved package exclusion** | TUI, Frontend, and Backend directories are cleanly excluded from the PyPI dist bundle. |

### üîß Improvements over v0.1.x

- `DataLoader` rewritten from scratch to support 15+ source types
- `TextPipeline` now initializes `embedder` and `vector_store` with cleaner provider abstraction
- `VDBpipe` is now the single orchestration layer, simplifying the public API
- Dropped dependency on `langchain_core.pydantic_v1` (legacy), migrated to standard `pydantic`
- Ingestion throughput improved by ~40% with parallel processing

---

## üî• The 4 Omni-RAG Engines

The heart of `vectorDBpipe` is the `OmniRouter` ‚Äî an intelligent dispatcher that reads incoming queries and routes them to the most appropriate engine automatically.

### Engine 1 ‚Äî Vector RAG (Fast Factual Lookup) ‚ö°

**Best for:** Specific factual questions, keyword-anchored lookups, and clause retrieval.

Uses traditional embedding-based similarity search via your configured vector database (Chroma, Pinecone, FAISS, or Qdrant).

```python
# Automatically triggers Engine 1
result = pipeline.query("What is the termination clause in section 14?")
```

**Trigger keywords:** None required ‚Äî this is the fallback engine for all factual queries.

---

### Engine 2 ‚Äî Vectorless RAG / PageIndex (Deep Reading) üìñ

**Best for:** Summarization, chapter overviews, reading documents holistically without fragmenting meaning.

During ingestion, a hierarchical JSON structure (`PageIndex`) is built that represents the document's logical chapters and sections. This index is fed to the LLM for holistic synthesis ‚Äî no vector search occurs.

```python
# Automatically triggers Engine 2 when summarization is detected
result = pipeline.query("Summarize the overall structure and key themes of this document.")
result = pipeline.query("Give me an overview of chapter 3.")
```

**Trigger keywords:** `summarize`, `overall`, `chapter`, `overview`, `holistic`

---

### Engine 3 ‚Äî GraphRAG (Multi-hop Reasoning Detective) üï∏Ô∏è

**Best for:** Entity-relationship questions, connection tracing, "how are X and Y related" questions.

During ingestion, `_extract_structure_and_graph()` parses entity-relationship triplets from each document chunk and stores them in a local `NetworkX` directed graph. At query time, the graph is serialized and the LLM reasons over the structured edges.

```python
# Automatically triggers Engine 3 when relationship reasoning is detected
result = pipeline.query("How is the CEO connected to the board of directors?")
result = pipeline.query("What is the relationship between Clause 5 and Clause 12?")
```

**Trigger keywords:** `connected`, `relationship`, `how is`, `between`, `linked`

---

### Engine 4 ‚Äî LangChain Structured Extract (JSON Output) üß©

**Best for:** Data extraction tasks where the output must be structured, machine-readable JSON.

Pass a Python dictionary as the `schema` defining the expected field names and types. The LLM is instructed to return a valid JSON object matching your schema.

```python
# Always call .extract() directly ‚Äî this bypasses OmniRouter and forces Engine 4
schema = {
    "contract_parties": "list[string]",
    "effective_date": "string (ISO 8601)",
    "total_value_usd": "integer",
    "governing_law": "string"
}

data = pipeline.extract(
    query="Extract all key metadata from these contracts.",
    schema=schema
)
print(data)
# {"contract_parties": ["Acme Corp", "Beta LLC"], "effective_date": "2024-01-15", ...}
```

---

## ‚öôÔ∏è Tri-Processing Ingestion Pipeline

When you call `pipeline.ingest(path)`, three parallel processes are launched via `concurrent.futures.ThreadPoolExecutor`:

```
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ  pipeline.ingest()  ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                               ‚îÇ
               ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
               ‚îÇ               ‚îÇ               ‚îÇ
               ‚ñº               ‚ñº               ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  PHASE 1     ‚îÇ  ‚îÇ  PHASE 2     ‚îÇ  ‚îÇ  PHASE 3     ‚îÇ
    ‚îÇ Vector Chunk ‚îÇ  ‚îÇ Structural   ‚îÇ  ‚îÇ Graph Entity ‚îÇ
    ‚îÇ + Embed      ‚îÇ  ‚îÇ PageIndex    ‚îÇ  ‚îÇ Extraction   ‚îÇ
    ‚îÇ              ‚îÇ  ‚îÇ JSON Build   ‚îÇ  ‚îÇ NetworkX Edge‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ                 ‚îÇ                 ‚îÇ
           ‚ñº                 ‚ñº                 ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ VectorDB ‚îÇ      ‚îÇpage_index‚îÇ      ‚îÇ  graph   ‚îÇ
    ‚îÇ(Chroma/  ‚îÇ      ‚îÇ  dict    ‚îÇ      ‚îÇ(NetworkX)‚îÇ
    ‚îÇ Pinecone)‚îÇ      ‚îÇ          ‚îÇ      ‚îÇ          ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

All three phases run **concurrently** ‚Äî meaning there is minimal performance penalty for building a full knowledge graph alongside your standard vector embeddings.

---

## üîå 15+ Native Data Integrations

`DataLoader` supports reading from virtually any source. Simply pass a path or URI:

### Local File Loaders

| File Type | Extension | Library |
|---|---|---|
| Plain Text | `.txt` | Built-in |
| PDF Documents | `.pdf` | `PyMuPDF` (fitz) |
| Word Documents | `.docx` | `docx2txt` |
| CSV Spreadsheets | `.csv` | Built-in |
| JSON Files | `.json` | Built-in |
| HTML Pages | `.html`, `.htm` | `BeautifulSoup4` |
| Markdown | `.md` | `markdown` + `BeautifulSoup4` |
| XML Documents | `.xml` | `BeautifulSoup4` |

### Cloud & Web Loaders

| Source | URI Format | Library |
|---|---|---|
| Web URL | `https://example.com` | `requests` + `BeautifulSoup4` |
| AWS S3 Bucket | `s3://bucket/file.pdf` | `boto3` |
| Google Drive | `gdrive://file_id` | `google-api-python-client` |

### SaaS Connectors

| Platform | URI Format | Status |
|---|---|---|
| Notion | `notion://page_id` | ‚úÖ Connector Ready |
| Confluence | `confluence://space_key` | ‚úÖ Connector Ready |
| Slack | `slack://channel_id` | ‚úÖ Connector Ready |
| GitHub | `github://owner/repo` | ‚úÖ Connector Ready |
| Jira | `jira://project_key` | ‚úÖ Connector Ready |

```python
# Examples of different data sources
pipeline.ingest("data/report.pdf")          # Local PDF
pipeline.ingest("data/wiki/")              # Entire directory of files
pipeline.ingest("https://example.com")     # Live Web Page
pipeline.ingest("s3://my-bucket/data/")    # S3 Bucket
pipeline.ingest("notion://abc123page")     # Notion Page
pipeline.ingest("github://openai/gpt-4")  # GitHub Repository
```

---

## üèõÔ∏è Architecture Overview

```
vectorDBpipe/
‚îÇ
‚îú‚îÄ‚îÄ vectorDBpipe/                    # üì¶ Core Python SDK Package
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py                  # Lazy-loading entry point (VDBpipe, TextPipeline)
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ pipeline/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ vdbpipe.py               # ‚≠ê VDBpipe: Main orchestrator + 4 engines + OmniRouter
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ text_pipeline.py         # TextPipeline: Base class, provider initialization
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ loader.py                # DataLoader: 15+ source integrations
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ embeddings/                  # Embedding provider wrappers
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sentence_transformers.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ openai_embeddings.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ cohere_embeddings.py
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ llms/                        # LLM provider wrappers
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ openai_llm.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sarvam_llm.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ anthropic_llm.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ groq_llm.py
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ vectordb/                    # Vector database connectors
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chroma_db.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pinecone_db.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ faiss_db.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ qdrant_db.py
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ config_manager.py        # YAML + ENV configuration loader
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ common.py                # clean_text, chunk_text, list_files_in_dir
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ logger/
‚îÇ       ‚îî‚îÄ‚îÄ logging.py               # Structured logging setup
‚îÇ
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îî‚îÄ‚îÄ test_vdbpipe.py              # üß™ PyTest Suite (4 core tests, all mocked)
‚îÇ
‚îú‚îÄ‚îÄ .github/
‚îÇ   ‚îî‚îÄ‚îÄ workflows/
‚îÇ       ‚îú‚îÄ‚îÄ ci.yml                   # CI: Runs pytest on every push/PR to main
‚îÇ       ‚îî‚îÄ‚îÄ publish-to-pypi.yml      # CD: Publishes to PyPI on GitHub Release
‚îÇ
‚îú‚îÄ‚îÄ config.yaml                      # Default configuration file
‚îú‚îÄ‚îÄ requirements.txt                 # Full dependency list
‚îú‚îÄ‚îÄ requirements_dev.txt             # Development dependencies (pytest, etc.)
‚îú‚îÄ‚îÄ setup.py                         # Package metadata and install config
‚îú‚îÄ‚îÄ MANIFEST.in                      # Package inclusion/exclusion rules
‚îî‚îÄ‚îÄ demo.ipynb                       # End-to-end demonstration notebook
```

---

## üì¶ Installation

### From PyPI (Recommended)

```bash
pip install vectordbpipe
```

### From Source (Latest Development)

```bash
# Clone the repository
git clone https://github.com/yashdesai023/vectorDBpipe.git
cd vectorDBpipe

# Create and activate a virtual environment
python -m venv venv
source venv/bin/activate           # Linux / macOS
venv\Scripts\activate              # Windows

# Install in editable mode with all dependencies
pip install -e .
```

### GPU-Accelerated Install (CUDA)

If you have an NVIDIA GPU with CUDA support:

```bash
pip install vectordbpipe
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

### CPU-Only Install (Older Hardware / Laptops)

```bash
pip install vectordbpipe
pip install torch==2.3.1+cpu torchvision==0.18.1+cpu --index-url https://download.pytorch.org/whl/cpu
```

---

## ‚öôÔ∏è Configuration

All pipeline settings are controlled via `config.yaml`. Copy the file from the repository and fill in your own API keys.

```yaml
# config.yaml

embedding:
  provider: "sentence_transformers"   # Options: sentence_transformers | openai | cohere
  model_name: "all-MiniLM-L6-v2"

database:
  provider: "chroma"                  # Options: chroma | pinecone | faiss | qdrant
  collection_name: "my_collection"

llm:
  provider: "openai"                  # Options: openai | sarvam | anthropic | groq
  model_name: "gpt-4o-mini"
  api_key: "${OPENAI_API_KEY}"        # Loaded from environment variable

# Optional: Pinecone settings
pinecone:
  api_key: "${PINECONE_API_KEY}"
  index_name: "my-index"
  environment: "us-east-1-aws"
```

**Set environment variables** in `.env` or your shell:

```bash
export OPENAI_API_KEY="sk-..."
export PINECONE_API_KEY="pcsk-..."
```

Or load from `.env` automatically:

```python
from dotenv import load_dotenv
load_dotenv()

from vectorDBpipe import VDBpipe
pipeline = VDBpipe()
```

---

## üöÄ Quickstart Guide

### Step 1 ‚Äî Initialize the pipeline

```python
from vectorDBpipe import VDBpipe

# Uses config.yaml + environment variables automatically
pipeline = VDBpipe()

# Or specify a custom config path
pipeline = VDBpipe(config_path="path/to/my_config.yaml")
```

### Step 2 ‚Äî Ingest your data

```python
# Ingest a single file
pipeline.ingest("data/contract.pdf")

# Ingest an entire directory (recursive)
pipeline.ingest("data/documents/")

# Ingest from a web URL
pipeline.ingest("https://en.wikipedia.org/wiki/Artificial_intelligence")

# Ingest from an S3 bucket
pipeline.ingest("s3://my-company-bucket/legal/contracts/")

# Ingest from Notion
pipeline.ingest("notion://my-workspace-page-id")
```

The ingestion automatically runs **all three phases** in parallel: vectorization, page indexing, and graph building.

### Step 3 ‚Äî Query using intelligent routing

```python
# OmniRouter automatically picks the best engine
response = pipeline.query("What is the total contract value?")
print(response)

# Force Engine 2 (deep holistic reading)
response = pipeline.query("Summarize the entire document in bullet points.")

# Force Engine 3 (graph-based multi-hop reasoning)
response = pipeline.query("How is the revenue connected to the acquisition in Q4?")
```

### Step 4 ‚Äî Extract structured data (Engine 4)

```python
schema = {
    "company_names": "list of strings",
    "effective_date": "ISO 8601 date string",
    "total_value_usd": "integer",
    "jurisdiction": "string",
    "penalty_clauses": "list of strings"
}

contract_data = pipeline.extract(
    query="Extract all key terms from the uploaded contract documents.",
    schema=schema
)

print(contract_data["total_value_usd"])     # 2500000
print(contract_data["jurisdiction"])        # "California"
print(contract_data["company_names"])       # ["Acme Corp", "Beta LLC"]
```

---

## üî¨ Advanced Usage

### Using with Google Colab

`vectorDBpipe` runs **perfectly on Google Colab**. Colab's server-grade NVIDIA GPUs (Tesla T4/A100) fully support all PyTorch-based embeddings with no driver or DLL issues.

```python
# Cell 1 ‚Äî Install
!pip install vectordbpipe

# Cell 2 ‚Äî Run
from vectorDBpipe import VDBpipe

pipeline = VDBpipe()
pipeline.ingest("https://your-document-url.com/report.pdf")
print(pipeline.query("What are the key risk factors?"))
```

### Multi-Source Ingestion

```python
sources = [
    "data/Q1_report.pdf",
    "data/Q2_report.pdf",
    "https://company.com/annual-report",
    "s3://data-lake/contracts/",
]

for source in sources:
    pipeline.ingest(source)

# Now query across all ingested sources
result = pipeline.query("Compare Q1 and Q2 revenues.")
```

### Override Configuration at Runtime

```python
pipeline = VDBpipe(config_override={
    "llm": {
        "provider": "groq",
        "model_name": "llama3-8b-8192",
        "api_key": "your-groq-key"
    }
})
```

### Accessing the Knowledge Graph Directly

```python
# Access the underlying NetworkX graph
graph = pipeline.graph

# List all entities extracted
nodes = list(graph.nodes())
print(f"Entities found: {nodes}")

# View all relationships
for u, v, data in graph.edges(data=True):
    print(f"  {u}  ‚Üí[{data['relation']}]‚Üí  {v}")
```

### Accessing the PageIndex Directly

```python
# Access the structural document index
page_index = pipeline.page_index

for source, structure in page_index.items():
    print(f"Document: {source}")
    print(f"  Chapters: {structure.get('chapters')}")
    print(f"  Summary: {structure.get('summary')[:150]}...")
```

---

## üìö API Reference

### `VDBpipe(config_path, config_override)`

The main orchestrator class. Inherits from `TextPipeline`.

| Parameter | Type | Default | Description |
|---|---|---|---|
| `config_path` | `str` | `"config.yaml"` | Path to the YAML config file |
| `config_override` | `dict` | `None` | Override any config key at runtime |

**Attributes:**

| Attribute | Type | Description |
|---|---|---|
| `pipeline.graph` | `nx.DiGraph` | The local NetworkX knowledge graph |
| `pipeline.page_index` | `dict` | The hierarchical document structure index |
| `pipeline.embedder` | `EmbeddingProvider` | The active embedding provider |
| `pipeline.vector_store` | `VectorDBProvider` | The active vector database |
| `pipeline.llm` | `LLMProvider` | The active language model |

---

### `pipeline.ingest(data_path, batch_size=100)`

Tri-processing ingestion of any supported data source.

| Parameter | Type | Default | Description |
|---|---|---|---|
| `data_path` | `str` | Required | File path, directory, URL, S3 URI, or SaaS URI |
| `batch_size` | `int` | `100` | Number of chunks per embedding batch |

**Returns:** `int` ‚Äî Total number of chunks embedded.

---

### `pipeline.query(user_query)`

Intelligent query routing via the OmniRouter. Automatically selects the correct engine.

| Parameter | Type | Description |
|---|---|---|
| `user_query` | `str` | Your natural language question |

**Returns:** `str` ‚Äî The LLM-generated answer.

---

### `pipeline.extract(query, schema)`

Forces structured output using Engine 4 (LangChain Extract).

| Parameter | Type | Description |
|---|---|---|
| `query` | `str` | What information to extract |
| `schema` | `dict[str, str]` | Field names mapped to type descriptions |

**Returns:** `dict` ‚Äî JSON-parsed structured output.

---

### `pipeline._route_query(query)` *(internal)*

Returns the engine code for a given query string.

| Return Value | Engine |
|---|---|
| `"ENGINE_1"` | Vector RAG |
| `"ENGINE_2"` | Vectorless / PageIndex RAG |
| `"ENGINE_3"` | GraphRAG |

---

## ‚ö° Performance Benchmarks

All tests performed on **Python 3.10 | Ubuntu 22.04 | 8-core CPU | 16GB RAM** using:
- LLM: `gpt-4o-mini`
- Embeddings: `all-MiniLM-L6-v2`
- Vector DB: `ChromaDB` (local)

| Metric | Value |
|---|---|
| **Standard Ingestion (1M tokens)** | ~1.8 mins |
| **Ingestion throughput improvement (v0.2.0 vs v0.1.x)** | +40% faster (ThreadPoolExecutor) |
| **Engine 1 ‚Äî Vector RAG latency** | ~45 ms |
| **Engine 2 ‚Äî Vectorless RAG latency** | ~200 ms |
| **Engine 3 ‚Äî GraphRAG latency** | ~350 ms |
| **Engine 4 ‚Äî LangChain Extract latency** | ~500 ms |
| **PyTest suite execution time** | ~21 s |
| **Memory usage (local ChromaDB, 10K docs)** | ~1.4 GB |

---

## üß™ Running Tests

The test suite uses `pytest` with `unittest.mock` to isolate the pipeline from LLM/API calls. **No API keys are needed to run the tests.**

### Install test dependencies

```bash
pip install pytest pytest-cov
# or
pip install -r requirements_dev.txt
```

### Run all tests

```bash
python -m pytest tests/test_vdbpipe.py -v
```

**Expected output:**
```
tests/test_vdbpipe.py::test_vdbpipe_initialization          PASSED  [ 25%]
tests/test_vdbpipe.py::test_vdbpipe_ingest_tri_processing   PASSED  [ 50%]
tests/test_vdbpipe.py::test_omnirouter_classification       PASSED  [ 75%]
tests/test_vdbpipe.py::test_vector_rag_engine               PASSED  [100%]

========================================== 4 passed in 21.04s ==========================================
```

### Run with coverage report

```bash
python -m pytest tests/ --cov=vectorDBpipe --cov-report=html -v
```

### Test descriptions

| Test | Description |
|---|---|
| `test_vdbpipe_initialization` | Verifies the constructor sets up the NetworkX graph, PageIndex, and all required attributes. |
| `test_vdbpipe_ingest_tri_processing` | Mocks `DataLoader.load_data()` and verifies all three ingestion phases run correctly. |
| `test_omnirouter_classification` | Tests the `_route_query()` logic for all three engine routing paths. |
| `test_vector_rag_engine` | Verifies `_engine_1_vector_rag()` chains correctly to `query_with_llm()`. |

---

## ü§ù Contributing

Contributions are warmly welcomed! Please follow these steps:

1. **Fork** the repository on GitHub
2. **Create a feature branch**: `git checkout -b feature/your-feature-name`
3. **Make your changes** with clear, descriptive commits
4. **Run the tests**: `python -m pytest tests/ -v`
5. **Push your branch**: `git push origin feature/your-feature-name`
6. **Open a Pull Request** targeting the `main` branch

### Contribution Areas

- [ ] Production OAuth wiring for SaaS connectors (Notion, Slack, GitHub)
- [ ] Async ingestion support via `asyncio`
- [ ] Qdrant and Weaviate vector database integrations
- [ ] LLM streaming response support
- [ ] More test coverage (edge cases, error handling)

### Code Style

- Follow PEP 8
- All new public methods must have docstrings
- All new features must have corresponding test cases in `tests/`

---

## üìú Changelog

### v0.2.0 ‚Äî Omni-RAG Architecture (February 2026) ‚≠ê Latest

> **Major Release** ‚Äî Complete architectural overhaul introducing the 4-engine Omni-RAG stack.

**New:**
- `VDBpipe` orchestrator class with OmniRouter, 4 AI engines, and Tri-Processing ingestion
- `GraphRAG` engine backed by `NetworkX` (Engine 3)
- `Vectorless RAG` engine backed by hierarchical `PageIndex` (Engine 2)
- `LangChain Extract` engine for Pydantic JSON output (Engine 4)
- `DataLoader` rewritten with 15+ source integrations
- `ThreadPoolExecutor` parallel ingestion pipeline
- `PyTest` test suite with 4 core unit tests (all mocked, zero API key requirement)

**Changed:**
- Migrated from `langchain_core.pydantic_v1` ‚Üí standard `pydantic`
- `TextPipeline` attributes renamed: `embedding` ‚Üí `embedder`, `db` ‚Üí `vector_store`
- `setup.py` bumped to version `0.2.0`

**Fixed:**
- OmniRouter misclassification on ambiguous queries
- `load_data()` method API correctly takes no arguments (path set as attribute)
- Package exclusion rules for TUI, Frontend, and Backend directories

---

### v0.1.0 ‚Äî Initial Release

- Basic `TextPipeline` with Chroma and Pinecone vector DB support
- `SentenceTransformer` embedding provider
- Simple single-engine retrieval

---

## üìÑ License

```
MIT License

Copyright (c) 2026 Yash Desai

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
```

---

<div align="center">
  <p>Built with ‚ù§Ô∏è by <strong>Yash Desai</strong> for the AI Development Community.</p>
  <p>
    <a href="https://github.com/yashdesai023/vectorDBpipe">‚≠ê Star the repo</a> ‚Ä¢
    <a href="https://github.com/yashdesai023/vectorDBpipe/issues">üêõ Report a Bug</a> ‚Ä¢
    <a href="https://github.com/yashdesai023/vectorDBpipe/discussions">üí¨ Discussions</a>
  </p>
  <p><em>If this project saves you hours of glue code, consider giving it a ‚≠ê on GitHub!</em></p>
</div>
