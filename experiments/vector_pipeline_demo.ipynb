{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üöÄ VectorDBPipe: End-to-End RAG Pipeline\n",
                "\n",
                "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yashdesai023/vectorDBpipe/blob/main/experiments/vector_pipeline_demo.ipynb)\n",
                "\n",
                "Welcome to the **vectorDBpipe** demo! \n",
                "\n",
                "In this notebook, we will demonstrate how to build a production-ready **Retrieval-Augmented Generation (RAG)** pipeline in less than 5 minutes.\n",
                "\n",
                "**What we will do:**\n",
                "1.  **Install** the library.\n",
                "2.  **Generate** dummy text data (representing your internal knowledge base).\n",
                "3.  **Ingest** the data (Clean -> Chunk -> Embed -> Store).\n",
                "4.  **Search** the data using semantic queries."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Installation\n",
                "\n",
                "First, we install the package directly from PyPI. \n",
                "> **Note:** If you are running on Google Colab, we add a few extra flags to ensure compatibility."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install vectorDBpipe\n",
                "!pip install vectordbpipe\n",
                "\n",
                "# If on Windows local machine without GPU, uncomment the line below:\n",
                "# !pip install -r https://github.com/yashdesai023/vectorDBpipe/raw/main/requirements-cpu.txt"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Setup & Data Generation\n",
                "\n",
                "We need some data to search! Let's create a temporary directory `my_data/` and add some text files representing corporate documents."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "# 1. Create Data Directory\n",
                "data_dir = \"my_data\"\n",
                "os.makedirs(data_dir, exist_ok=True)\n",
                "\n",
                "# 2. Create Dummy Files\n",
                "documents = {\n",
                "    \"policy_remote_work.txt\": \"\"\"\n",
                "    Remote Work Policy (2025):\n",
                "    Employees are allowed to work from home 3 days a week.\n",
                "    Core hours are 10 AM to 4 PM EST.\n",
                "    Approval from the manager is required for full-time remote work.\n",
                "    \"\"\",\n",
                "    \"project_alpha_specs.txt\": \"\"\"\n",
                "    Project Alpha Specifications:\n",
                "    The goal is to build a scalable vector search engine.\n",
                "    It must support Pinecone and ChromaDB.\n",
                "    The latency for search queries should be under 200ms.\n",
                "    \"\"\",\n",
                "    \"meeting_minutes_jan.txt\": \"\"\"\n",
                "    Minutes of Meeting - Jan 15:\n",
                "    - Discussed the budget for Q1.\n",
                "    - Approved the purchase of new H100 GPUs.\n",
                "    - Team outing scheduled for Feb 20th at the Central Park.\n",
                "    \"\"\"\n",
                "}\n",
                "\n",
                "for filename, content in documents.items():\n",
                "    with open(os.path.join(data_dir, filename), \"w\", encoding=\"utf-8\") as f:\n",
                "        f.write(content.strip())\n",
                "\n",
                "print(f\"‚úÖ Generated {len(documents)} sample documents in '{data_dir}'\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b54c2595",
            "metadata": {},
            "source": [
                "### üí° Pro Tip: Customizing `config.yaml`\n",
                "\n",
                "In a real project, you would edit `vectorDBpipe/config/config.yaml`. Here is a quick guide on key settings:\n",
                "\n",
                "*   **`model.name`**: Switch to a larger model like `sentence-transformers/all-mpnet-base-v2` for better accuracy.\n",
                "*   **`vector_db.type`**: Change from `\"chroma\"` (local) to `\"pinecone\"` (cloud serverless) for production.\n",
                "*   **`paths.data_dir`**: Point this to your actual document folder (e.g., `\"/content/my_pdfs/\"`)."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "811aa45f",
            "metadata": {},
            "source": [
                "## 3. Initialize the Pipeline\n",
                "\n",
                "We use `TextPipeline` as our main entry point. \n",
                "\n",
                "By default, it looks for a `config.yaml`. In this demo, we create one dynamically:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from vectorDBpipe.pipeline.text_pipeline import TextPipeline\n",
                "import yaml\n",
                "\n",
                "# Optional: Create a custom config file to point to our new data folder\n",
                "config_content = \"\"\"\n",
                "paths:\n",
                "  data_dir: \"my_data/\"\n",
                "  logs_dir: \"logs/\"\n",
                "\n",
                "model:\n",
                "  name: \"sentence-transformers/all-MiniLM-L6-v2\"\n",
                "\n",
                "vector_db:\n",
                "  type: \"chroma\"  # Using local ChromaDB for this demo\n",
                "  persist_directory: \"chroma_db_storage\"\n",
                "\"\"\"\n",
                "\n",
                "with open(\"demo_config.yaml\", \"w\") as f:\n",
                "    f.write(config_content)\n",
                "\n",
                "# Initialize Pipeline with our custom config\n",
                "pipeline = TextPipeline(config_path=\"demo_config.yaml\")\n",
                "print(\"‚úÖ Pipeline Initialized!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Run Ingestion (Load -> Clean -> Chunk -> Embed -> Store)\n",
                "\n",
                "This is the magic step. We call `process()` to read all files in `my_data/`, turn them into vectors, and store them in ChromaDB.\n",
                "\n",
                "We use `batch_size=10` here because our data is small, but for large datasets, use `100` or more."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pipeline.process(batch_size=10)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Semantic Search\n",
                "\n",
                "Now that our data is indexed, let's ask questions! Notice we don't need exact keyword matches. \n",
                "\n",
                "*   **Query 1**: \"Can I work from home?\" (Matches *Remote Work Policy*)\n",
                "*   **Query 2**: \"What GPUs are we buying?\" (Matches *Meeting Minutes*)\n",
                "*   **Query 3**: \"What are the requirements for Project Alpha?\" (Matches *Project Specs*)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def print_results(results):\n",
                "    print(\"\\n--- üîç Search Results ---\")\n",
                "    for i, res in enumerate(results):\n",
                "        meta = res.get('metadata', {})\n",
                "        print(f\"Result {i+1} (Source: {meta.get('source', 'Unknown')}):\")\n",
                "        print(f\"  \\\"...{meta.get('text', '')}...\\\"\\n\")\n",
                "\n",
                "# Test Query 1\n",
                "q1 = \"Can I work from home on Fridays?\"\n",
                "print(f\"Query: {q1}\")\n",
                "results = pipeline.search(q1, top_k=1)\n",
                "print_results(results)\n",
                "\n",
                "# Test Query 2\n",
                "q2 = \"What did we decide about new hardware?\"\n",
                "print(f\"Query: {q2}\")\n",
                "results = pipeline.search(q2, top_k=1)\n",
                "print_results(results)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. What's Next?\n",
                "\n",
                "You have successfully built a Search Engine!\n",
                "\n",
                "**To go to Production:**\n",
                "1.  Change `vector_db.type` to `\"pinecone\"` in `config.yaml`.\n",
                "2.  Set your `PINECONE_API_KEY` environment variable.\n",
                "3.  Point `data_dir` to your real Gigabyte-scale dataset.\n",
                "4.  Run `pipeline.process(batch_size=100)`.\n",
                "\n",
                "Enjoy using **vectorDBpipe**! üöÄ"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
