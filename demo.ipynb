{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ vectorDBpipe ‚Äî Omni-RAG Demo\n",
    "\n",
    "**v0.2.1** | [GitHub](https://github.com/vectordbpipe/vectorDBpipe) | [PyPI](https://pypi.org/project/vectordbpipe/)\n",
    "\n",
    "This notebook demonstrates the full Omni-RAG architecture:\n",
    "- ‚úÖ **Tri-Processing Ingestion** ‚Äî Vector, PageIndex, and GraphRAG simultaneously\n",
    "- ‚úÖ **OmniRouter** ‚Äî Automatic engine selection per query type\n",
    "- ‚úÖ **4 RAG Engines** ‚Äî Vector, Vectorless, GraphRAG, LangChain Extract\n",
    "- ‚úÖ **15+ Data Sources** ‚Äî PDF, DOCX, S3, Notion, GitHub, Slack, and more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Step 1 ‚Äî Install the Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the latest version\n",
    "!pip install vectordbpipe==0.2.1 -q\n",
    "print('‚úÖ vectordbpipe installed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Step 2 ‚Äî Verify Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')\n",
    "\n",
    "from vectorDBpipe import VDBpipe\n",
    "print('‚úÖ VDBpipe imported successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Step 3 ‚Äî Create Demo Data\n",
    "\n",
    "We create a small sample text file to demonstrate ingestion. In production, point this at a real PDF, S3 bucket, or Notion page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('demo_data', exist_ok=True)\n",
    "\n",
    "sample_text = \"\"\"\n",
    "# Q3 2024 Financial Report ‚Äî Acme Corporation\n",
    "\n",
    "## Executive Summary\n",
    "Acme Corporation achieved record revenue of $2.5 billion in Q3 2024,\n",
    "representing a 23% year-over-year growth. The growth was primarily\n",
    "driven by the acquisition of Startup X in July 2024.\n",
    "\n",
    "## Key Executives\n",
    "- CEO: John Smith, who joined in 2019\n",
    "- CFO: Sarah Johnson, responsible for the Q4 acquisition strategy\n",
    "- CTO: Michael Chen, leading the AI transformation initiative\n",
    "\n",
    "## Financial Highlights\n",
    "- Total Revenue: $2.5 billion (Q3 2024)\n",
    "- Net Profit: $450 million\n",
    "- Operating Margin: 18%\n",
    "- Cash Reserves: $800 million\n",
    "\n",
    "## Risk Factors\n",
    "The primary risk factors include supply chain disruptions in Asia,\n",
    "regulatory changes in the European markets, and competition from\n",
    "Tech Giant Corp.\n",
    "\n",
    "## Governance\n",
    "The Board of Directors is chaired by Dr. Emily Watson. John Smith\n",
    "reports directly to the board. The penalty for any breach of fiduciary\n",
    "duty is $5 million as per Section 14 of the corporate charter.\n",
    "\"\"\"\n",
    "\n",
    "with open('demo_data/q3_report.txt', 'w') as f:\n",
    "    f.write(sample_text)\n",
    "\n",
    "print('‚úÖ Demo data created at demo_data/q3_report.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Step 4 ‚Äî Initialize VDBpipe with Config Override\n",
    "\n",
    "Use `config_override` to set providers at runtime ‚Äî **no `config.yaml` file needed on Colab!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Option A: Use a FREE local configuration (no API keys needed)\n",
    "# - Embeddings: SentenceTransformers (all-MiniLM-L6-v2)\n",
    "# - Vector DB: FAISS (local, in-memory)\n",
    "# - LLM: None (RAG without generation ‚Äî retrieval only)\n",
    "# ============================================================\n",
    "\n",
    "pipeline = VDBpipe(config_override={\n",
    "    \"embedding\": {\n",
    "        \"provider\": \"local\",\n",
    "        \"model_name\": \"all-MiniLM-L6-v2\"\n",
    "    },\n",
    "    \"database\": {\n",
    "        \"provider\": \"faiss\",\n",
    "        \"mode\": \"local\",\n",
    "        \"collection_name\": \"demo_collection\"\n",
    "    },\n",
    "    \"llm\": {\n",
    "        \"provider\": \"null\"\n",
    "    },\n",
    "    \"paths\": {\n",
    "        \"logs_dir\": \"logs/\",\n",
    "        \"data_dir\": \"demo_data/\"\n",
    "    }\n",
    "})\n",
    "\n",
    "print('‚úÖ VDBpipe initialized successfully!')\n",
    "print(f'   Graph: {pipeline.graph}')\n",
    "print(f'   PageIndex: {pipeline.page_index}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Step 5 ‚Äî Tri-Processing Ingestion\n",
    "\n",
    "One call to `ingest()` runs **3 parallel pipelines**:\n",
    "1. üóÇÔ∏è Chunks text and stores embeddings in FAISS\n",
    "2. üìñ Builds a hierarchical PageIndex JSON structure\n",
    "3. üï∏Ô∏è Extracts entities and relationships into a NetworkX graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.ingest('demo_data/')\n",
    "\n",
    "print('\\n‚úÖ Ingestion complete!')\n",
    "print(f'   Graph nodes: {list(pipeline.graph.nodes())}')\n",
    "print(f'   PageIndex keys: {list(pipeline.page_index.keys())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Step 6 ‚Äî OmniRouter Query (Retrieval without LLM)\n",
    "\n",
    "Since we set `llm.provider: null`, we get ranked retrieval results back.\n",
    "To get LLM-generated answers, set your OpenAI/Groq/Anthropic key in the config override."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The OmniRouter classifies these queries and picks the right engine:\n",
    "\n",
    "# Engine 1 ‚Äî Vector RAG (direct factual lookup)\n",
    "result1 = pipeline.query(\"What was the total revenue in Q3 2024?\")\n",
    "print('Engine 1 (Vector RAG):')\n",
    "print(result1)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Engine 2 ‚Äî Vectorless / PageIndex RAG (holistic reading)\n",
    "result2 = pipeline.query(\"Summarize the overall document.\")\n",
    "print('Engine 2 (Vectorless RAG):')\n",
    "print(result2)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Engine 3 ‚Äî GraphRAG (relationship reasoning)\n",
    "result3 = pipeline.query(\"How is the CEO connected to the board?\")\n",
    "print('Engine 3 (GraphRAG):')\n",
    "print(result3)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß© Step 7 ‚Äî (Optional) Use with OpenAI for Full RAG Generation\n",
    "\n",
    "If you have an OpenAI API key, set it and re-initialize to get full LLM-generated answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and set your API key to enable LLM generation:\n",
    "\n",
    "# import os\n",
    "# os.environ['OPENAI_API_KEY'] = 'sk-...your-key-here...'\n",
    "\n",
    "# pipeline_gpt = VDBpipe(config_override={\n",
    "#     \"embedding\": {\"provider\": \"local\", \"model_name\": \"all-MiniLM-L6-v2\"},\n",
    "#     \"database\": {\"provider\": \"faiss\", \"mode\": \"local\", \"collection_name\": \"demo_gpt\"},\n",
    "#     \"llm\": {\"provider\": \"openai\", \"model_name\": \"gpt-4o-mini\"},\n",
    "#     \"paths\": {\"logs_dir\": \"logs/\", \"data_dir\": \"demo_data/\"}\n",
    "# })\n",
    "# pipeline_gpt.ingest('demo_data/')\n",
    "# answer = pipeline_gpt.query(\"What was Q3 revenue and who is CEO?\")\n",
    "# print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 8 ‚Äî Extract Structured JSON (Engine 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Engine 4 works with an LLM. With llm=null it returns the retrieved context.\n",
    "# With GPT/Groq configured, it returns type-safe JSON.\n",
    "\n",
    "schema = {\n",
    "    \"company_name\": \"string\",\n",
    "    \"revenue_usd\": \"integer\",\n",
    "    \"ceo_name\": \"string\",\n",
    "    \"risk_factors\": \"list of strings\"\n",
    "}\n",
    "\n",
    "extracted = pipeline.extract(\n",
    "    query=\"Extract all key company metrics from the document.\",\n",
    "    schema=schema\n",
    ")\n",
    "print('üß© Extracted Data (Engine 4):')\n",
    "print(extracted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Summary\n",
    "\n",
    "| Feature | Status |\n",
    "|---|---|\n",
    "| Package Installation | ‚úÖ |\n",
    "| VDBpipe Initialization | ‚úÖ |\n",
    "| Tri-Processing Ingestion | ‚úÖ |\n",
    "| Engine 1 ‚Äî Vector RAG | ‚úÖ |\n",
    "| Engine 2 ‚Äî Vectorless RAG | ‚úÖ |\n",
    "| Engine 3 ‚Äî GraphRAG | ‚úÖ |\n",
    "| Engine 4 ‚Äî LangChain Extract | ‚úÖ (needs LLM for generation) |\n",
    "\n",
    "---\n",
    "*vectorDBpipe v0.2.1 | Created by Yash Desai | [GitHub](https://github.com/vectordbpipe/vectorDBpipe)*"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
